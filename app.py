import requests
import json
import time
from collections import defaultdict
from rediscluster import RedisCluster, RedisClusterException

# ======================================
# ‚öôÔ∏è C·∫•u h√¨nh
# ======================================
URL = "https://spine-mri-public-data.s3.ap-southeast-1.amazonaws.com/transformed/cleaned_mri_data.json"

# Redis Cluster nodes
startup_nodes = [
    {"host": "127.0.0.1", "port": "7001"},
    {"host": "127.0.0.1", "port": "7002"},
    {"host": "127.0.0.1", "port": "7003"},
]

r = RedisCluster(startup_nodes=startup_nodes, decode_responses=True, skip_full_coverage_check=True)

# ======================================
# üì• T·∫£i d·ªØ li·ªáu
# ======================================
def fetch_data(url):
    print("üîÑ ƒêang t·∫£i d·ªØ li·ªáu...")
    response = requests.get(url)
    response.raise_for_status()
    data = response.json()
    print(f"‚úÖ ƒê√£ t·∫£i {len(data)} b·∫£n ghi.")
    return data


# ======================================
# üß† Gom nh√≥m d·ªØ li·ªáu
# ======================================
def group_data(data):
    grouped = defaultdict(list)
    for item in data:
        pid = item.get("patient_id")
        sid = item.get("study_id")
        seid = item.get("series_id")
        if not all([pid, sid, seid]):
            continue
        # Hash tag {pid} ƒë·ªÉ key c√πng b·ªánh nh√¢n n·∫±m c√πng node
        key = f"patient:{{{pid}}}:{sid}:{seid}"
        grouped[key].append(item)
    print(f"‚úÖ Gom ƒë∆∞·ª£c {len(grouped)} nh√≥m d·ªØ li·ªáu.")
    # ======================================
    # üßÆ T√≠nh dung l∆∞·ª£ng t·ª´ng nh√≥m
    # ======================================
    total_size_bytes = 0
    for key, value in grouped.items():
        json_str = json.dumps(value)
        size_bytes = len(json_str.encode('utf-8'))
        size_mb = size_bytes / (1024 * 1024)
        total_size_bytes += size_bytes
        print(f"Key: {key}, Items: {len(value)}, Size: {size_mb:.2f} MB")

    total_size_mb = total_size_bytes / (1024 * 1024)
    print(f"\nüß† T·ªïng dung l∆∞·ª£ng d·ª± ki·∫øn cho {len(grouped)} nh√≥m: {total_size_mb:.2f} MB")
    return grouped


# ======================================
# üíæ Ghi tr·ª±c ti·∫øp t·ª´ng key v√†o Redis
# ======================================
def save_to_redis_direct(grouped):
    total_keys = len(grouped)
    print(f"üíæ B·∫Øt ƒë·∫ßu ghi {total_keys} keys v√†o Redis Cluster...")

    start_time = time.time()
    keys_written = 0

    for key, value in grouped.items():
        try:
            r.set(key, json.dumps(value))
            keys_written += 1
            if keys_written % 100 == 0 or keys_written == total_keys:
                print(f"  ‚úÖ ƒê√£ ghi {keys_written}/{total_keys} keys")
        except Exception as e:
            print(f"‚ùå L·ªói khi ghi key {key}: {e}")

    duration = time.time() - start_time
    rate = total_keys / duration if duration > 0 else total_keys
    print(f"‚úÖ Ho√†n t·∫•t ghi {total_keys} keys trong {duration:.2f}s ({rate:.2f} keys/s)")

    print_redis_memory(r)

def print_redis_memory(r):
    try:
        mem_info = r.info("memory")  # Tr·∫£ v·ªÅ dict theo node
        print("üß† Memory info per node:")
        for node, info in mem_info.items():
            # info l√† dict ch·ª©a used_memory, used_memory_human, ...
            used_human = info.get("used_memory_human", "N/A")
            print(f"  Node {node}: {used_human}")
    except Exception as e:
        print("‚ùå L·ªói khi l·∫•y memory info:", e)

# ======================================
# üöÄ MAIN
# ======================================
def main():
    data = fetch_data(URL)
    grouped = group_data(data)
    save_to_redis_direct(grouped)
    print("\nüéØ Ho√†n t·∫•t ghi d·ªØ li·ªáu v√†o Redis Cluster.")


if __name__ == "__main__":
    main()
