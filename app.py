import requests
import json
import time
from collections import defaultdict
from rediscluster import RedisCluster

# ======================================
# âš™ï¸ Cáº¥u hÃ¬nh
# ======================================
URL = "https://spine-mri-public-data.s3.ap-southeast-1.amazonaws.com/transformed/cleaned_mri_data.json"

# Redis Cluster nodes
startup_nodes = [
    {"host": "127.0.0.1", "port": "7000"},
    {"host": "127.0.0.1", "port": "7001"},
    {"host": "127.0.0.1", "port": "7002"},
]

# Káº¿t ná»‘i tá»›i Redis Cluster
r = RedisCluster(startup_nodes=startup_nodes, decode_responses=True)


# ======================================
# ğŸ“¥ Táº£i dá»¯ liá»‡u
# ======================================
def fetch_data(url):
    print("ğŸ”„ Äang táº£i dá»¯ liá»‡u...")
    response = requests.get(url)
    response.raise_for_status()
    data = response.json()
    print(f"âœ… ÄÃ£ táº£i {len(data)} báº£n ghi.")
    return data


# ======================================
# ğŸ§  Gom nhÃ³m dá»¯ liá»‡u
# ======================================
def group_data(data):
    grouped = defaultdict(list)
    for item in data:
        pid = item.get("patient_id")
        sid = item.get("study_id")
        seid = item.get("series_id")
        if not all([pid, sid, seid]):
            continue
        # DÃ¹ng hash tag {pid} Ä‘á»ƒ cÃ¡c key cÃ¹ng bá»‡nh nhÃ¢n náº±m cÃ¹ng node
        key = f"patient:{{{pid}}}:{sid}:{seid}"
        grouped[key].append(item)
    print(f"âœ… Gom Ä‘Æ°á»£c {len(grouped)} nhÃ³m dá»¯ liá»‡u.")
    return grouped


# ======================================
# ğŸ’¾ LÆ°u dá»¯ liá»‡u + benchmark
# ======================================
def save_to_redis_with_benchmark(grouped):
    total_keys = len(grouped)
    print(f"ğŸ’¾ Báº¯t Ä‘áº§u ghi {total_keys} nhÃ³m vÃ o Redis Cluster...")

    # --- Ghi + Ä‘o thá»i gian ---
    start_time = time.time()
    pipe = r.pipeline(transaction=False)
    count = 0
    batch_size = 500  # flush má»—i 500 keys Ä‘á»ƒ khÃ´ng quÃ¡ táº£i network

    for key, value in grouped.items():
        pipe.set(key, json.dumps(value))
        count += 1
        if count % batch_size == 0:
            pipe.execute()
    pipe.execute()  # flush pháº§n cuá»‘i

    duration = time.time() - start_time
    rate = total_keys / duration

    print(f"âœ… ÄÃ£ ghi {total_keys} keys trong {duration:.2f}s ({rate:.2f} keys/s)")

    # --- Dung lÆ°á»£ng bá»™ nhá»› ---
    mem_info = r.info("memory")
    print(f"ğŸ§  Tá»•ng dung lÆ°á»£ng Redis Ä‘ang dÃ¹ng: {mem_info['used_memory_human']}")

    return duration, rate


# ======================================
# ğŸ“Š Thá»‘ng kÃª phÃ¢n bá»• giá»¯a cÃ¡c node
# ======================================
def cluster_distribution():
    print("\nğŸ“Š PhÃ¢n bá»‘ dá»¯ liá»‡u giá»¯a cÃ¡c node:")
    cluster_nodes = r.cluster_nodes()
    masters = [n for n in cluster_nodes.values() if n['role'] == 'master']
    for node in masters:
        node_id = node['id']
        addr = node['addr']
        node_info = r.info('memory', target_nodes=node_id)
        used = node_info[node_id]['used_memory_human']
        print(f"  - Node {addr}: dÃ¹ng {used}")


# ======================================
# ğŸš€ MAIN
# ======================================
def main():
    data = fetch_data(URL)
    grouped = group_data(data)
    duration, rate = save_to_redis_with_benchmark(grouped)
    cluster_distribution()
    print("\nğŸ¯ HoÃ n táº¥t benchmark Redis Cluster vá»›i dá»¯ liá»‡u tháº­t.")


if __name__ == "__main__":
    main()
